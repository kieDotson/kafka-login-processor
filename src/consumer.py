#!/usr/bin/env python3
"""
Kafka consumer for login data processing.
Consumes data from the user-login topic, processes it, and produces to processed-logins topic.
"""
import json
import logging
import os
import time
from kafka import KafkaConsumer, KafkaProducer

# Import the processor
from processor import LoginDataProcessor

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration from environment variables
BOOTSTRAP_SERVERS = os.environ.get('BOOTSTRAP_SERVERS', 'kafka:9092')
INPUT_TOPIC = os.environ.get('INPUT_TOPIC', 'user-login')
OUTPUT_TOPIC = os.environ.get('OUTPUT_TOPIC', 'processed-logins')
CONSUMER_GROUP = os.environ.get('CONSUMER_GROUP', 'login-processor-group')


class KafkaLoginConsumer:
    """
    Consumes login data from Kafka, processes it, and produces the results to another topic.
    """
    
    def __init__(self, bootstrap_servers, input_topic, output_topic, consumer_group):
        """
        Initialize the consumer with Kafka connection parameters.
        
        Args:
            bootstrap_servers (str): Kafka bootstrap servers
            input_topic (str): Topic to consume from
            output_topic (str): Topic to produce to
            consumer_group (str): Consumer group ID
        """
        self.bootstrap_servers = bootstrap_servers
        self.input_topic = input_topic
        self.output_topic = output_topic
        self.consumer_group = consumer_group
        
        # Create processor instance
        self.processor = LoginDataProcessor(bootstrap_servers=bootstrap_servers)
        
        # Initialize Kafka consumer and producer
        self._init_kafka()
        
        # Flag to control the main loop
        self.running = True
    
    def _init_kafka(self):
        """Initialize Kafka consumer and producer with appropriate configurations."""
        try:
            # Initialize the consumer
            self.consumer = KafkaConsumer(
                self.input_topic,
                bootstrap_servers=self.bootstrap_servers,
                group_id=self.consumer_group,
                auto_offset_reset='earliest',
                value_deserializer=lambda m: json.loads(m.decode('utf-8')),
                enable_auto_commit=False  # We'll commit manually after processing
            )
            
            # Initialize the producer
            self.producer = KafkaProducer(
                bootstrap_servers=self.bootstrap_servers,
                value_serializer=lambda v: json.dumps(v).encode('utf-8')
            )
            
            logger.info(f"Connected to Kafka at {self.bootstrap_servers}")
            logger.info(f"Consuming from: {self.input_topic}")
            logger.info(f"Producing to: {self.output_topic}")
            
        except Exception as e:
            logger.error(f"Failed to initialize Kafka: {e}")
            raise
    
    def log_insights(self, insights):
        """
        Log insights from the processor.
        
        Args:
            insights (dict): Insights generated by the processor
        """
        if not insights:
            return
        
        logger.info("Current insights:")
        
        # Log top device types
        if 'top_device_types' in insights:
            devices_str = ', '.join(f"{d[0]}: {d[1]}" for d in insights['top_device_types'])
            logger.info(f"Top device types: {devices_str}")
        
        # Log top locales
        if 'top_locales' in insights:
            locales_str = ', '.join(f"{l[0]}: {l[1]}" for l in insights['top_locales'])
            logger.info(f"Top locales: {locales_str}")
        
        # Log top app versions
        if 'top_app_versions' in insights:
            versions_str = ', '.join(f"{v[0]}: {v[1]}" for v in insights['top_app_versions'])
            logger.info(f"Top app versions: {versions_str}")
        
        # Log peak hours
        if 'peak_hours' in insights:
            hours_str = ', '.join(f"{h[0]}:00: {h[1]}" for h in insights['peak_hours'])
            logger.info(f"Peak activity hours: {hours_str}")
        
        # Log processing stats
        if 'avg_processing_time_ms' in insights:
            logger.info(f"Average processing time: {insights['avg_processing_time_ms']:.2f} ms")
        
        if 'invalid_percentage' in insights:
            logger.info(f"Invalid message percentage: {insights['invalid_percentage']:.2f}%")
    
    def run(self):
        """
        Main processing loop.
        
        Continuously consumes messages from the input topic,
        processes them, and produces to the output topic.
        """
        try:
            logger.info("Starting processing loop")
            
            # Main processing loop
            for message in self.consumer:
                try:
                    # Process the message using the processor
                    processed_message, is_valid, insights = self.processor.process(message.value)
                    
                    # Log any insights
                    self.log_insights(insights)
                    
                    # If processing was successful, produce to output topic
                    if is_valid and processed_message:
                        logger.info(f"Processed message: {json.dumps(processed_message)}")
                        self.producer.send(self.output_topic, processed_message)
                    
                    # Commit the offset after processing
                    self.consumer.commit()
                    
                except Exception as e:
                    logger.error(f"Error processing message: {e}")
                
                # Check if we should continue running
                if not self.running:
                    break
                    
        except KeyboardInterrupt:
            logger.info("Interrupted. Closing consumer and producer...")
            self.running = False
        except Exception as e:
            logger.error(f"Error in processing loop: {e}")
        finally:
            # Clean up
            self.shutdown()
    
    def shutdown(self):
        """Clean shutdown of consumer and producer."""
        logger.info("Shutting down...")
        
        # Close the producer
        if hasattr(self, 'producer'):
            try:
                self.producer.flush()
                self.producer.close()
                logger.info("Producer closed")
            except Exception as e:
                logger.error(f"Error closing producer: {e}")
        
        # Close the consumer
        if hasattr(self, 'consumer'):
            try:
                self.consumer.close()
                logger.info("Consumer closed")
            except Exception as e:
                logger.error(f"Error closing consumer: {e}")
                

if __name__ == "__main__":
    # Wait a bit for Kafka to be fully up and running
    logger.info("Waiting for Kafka to be ready...")
    time.sleep(10)
    
    # Create and run the consumer
    consumer = KafkaLoginConsumer(
        bootstrap_servers=BOOTSTRAP_SERVERS,
        input_topic=INPUT_TOPIC,
        output_topic=OUTPUT_TOPIC,
        consumer_group=CONSUMER_GROUP
    )
    
    consumer.run()